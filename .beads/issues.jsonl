{"id":"kaggle_mallorn-0pf","title":"Quick Wins: Immediate Improvements","description":"## Low-effort, high-impact changes to implement first\n\n### 1. Fix in_features Mismatch in train.py\nCurrent `train.py` hardcodes `in_features=3` but dataset now produces 6 features:\n```python\n# Line 97-98 in scripts/train.py\nmodel = LightCurveCNN(\n    in_features=3,  # Should be 6!\n    ...\n)\n```\nSame issue for transformer at line 116.\n\n### 2. Power-Law Decay Fitting\nAdd proper t^α fitting with goodness-of-fit:\n```python\ndef fit_power_law_decay(time_post_peak, flux_post_peak):\n    '''Fit F = A * t^alpha, return alpha and chi2'''\n    if len(time_post_peak) \u003c 3:\n        return 0.0, np.inf\n    \n    log_t = np.log(time_post_peak + 1)\n    log_f = np.log(np.abs(flux_post_peak) + 1e-8)\n    \n    # Linear fit in log-log space\n    coeffs, residuals, _, _, _ = np.polyfit(log_t, log_f, 1, full=True)\n    alpha = coeffs[0]\n    \n    # Chi-squared\n    predicted = coeffs[0] * log_t + coeffs[1]\n    chi2 = np.sum((log_f - predicted)**2) / len(log_f)\n    \n    return alpha, chi2\n```\nTDEs should have α ≈ -1.67\n\n### 3. Color Evolution Features\n```python\ndef compute_color_evolution(band_data):\n    '''Compute color change rate'''\n    g_peak_time, g_peak_flux = get_peak(band_data['g'])\n    r_peak_time, r_peak_flux = get_peak(band_data['r'])\n    \n    # Color at peak\n    color_at_peak = -2.5 * np.log10(g_peak_flux / r_peak_flux)\n    \n    # Color at +30 days (interpolate)\n    g_30 = interpolate(band_data['g'], g_peak_time + 30)\n    r_30 = interpolate(band_data['r'], r_peak_time + 30)\n    color_at_30d = -2.5 * np.log10(g_30 / r_30)\n    \n    # Evolution rate\n    color_evolution_rate = (color_at_30d - color_at_peak) / 30\n    \n    return color_at_peak, color_at_30d, color_evolution_rate\n```\n\n### 4. Smoothness Features\n```python\ndef compute_smoothness(flux):\n    '''TDEs are smooth, SNe have bumps'''\n    # Second derivative (acceleration)\n    d2 = np.diff(flux, n=2)\n    \n    # Number of sign changes (inflection points)\n    sign_changes = np.sum(np.diff(np.sign(d2)) != 0)\n    \n    # Max absolute second derivative\n    max_accel = np.max(np.abs(d2))\n    \n    # Ratio of monotonic segments\n    increasing = np.sum(np.diff(flux) \u003e 0)\n    monotonic_ratio = max(increasing, len(flux) - 1 - increasing) / (len(flux) - 1)\n    \n    return sign_changes, max_accel, monotonic_ratio\n```\n\n### 5. Run Optuna on Current LightGBM\nQuick hyperparameter search:\n```python\ndef lgbm_objective(trial):\n    params = {\n        'num_leaves': trial.suggest_int('num_leaves', 15, 63),\n        'learning_rate': trial.suggest_float('lr', 0.01, 0.2, log=True),\n        'feature_fraction': trial.suggest_float('ff', 0.6, 1.0),\n        'bagging_fraction': trial.suggest_float('bf', 0.6, 1.0),\n        'min_child_samples': trial.suggest_int('mcs', 5, 30),\n        'reg_alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n    }\n    # CV and return F1\n```\n\n### 6. Ensemble Current Models Properly\nInstead of simple averaging, use learned weights:\n```python\n# Get OOF predictions from CNN, Transformer, LightGBM\noof_cnn = load('checkpoints/cnn/oof_preds.npy')\noof_transformer = load('checkpoints/transformer/oof_preds.npy')\noof_lgbm = load('checkpoints/lgbm/oof_preds.npy')\n\n# Stack and learn weights\nfrom sklearn.linear_model import LogisticRegression\nmeta_X = np.column_stack([oof_cnn, oof_transformer, oof_lgbm])\nmeta_model = LogisticRegression()\nmeta_model.fit(meta_X, y_train)\n# Weights in meta_model.coef_\n```","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T08:59:02.016671555+11:00","updated_at":"2025-12-23T09:31:13.19730732+11:00","closed_at":"2025-12-23T09:31:13.19730732+11:00","close_reason":"All 6 quick wins implemented: in_features fix, power-law decay, color evolution, smoothness features, Optuna tuning, stacking ensemble","labels":["priority:high","quick-win"]}
{"id":"kaggle_mallorn-1ty","title":"De-extinction correction implementation","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T09:53:10.929945+11:00","updated_at":"2025-12-22T10:07:57.130464+11:00","closed_at":"2025-12-22T10:07:57.130464+11:00"}
{"id":"kaggle_mallorn-2lp","title":"Phase 2: GP Interpolation + FATS Features","description":"## Expected Impact: 0.25 → 0.40\n\n### Gaussian Process Light Curve Modeling\nCurrent approach uses raw irregular samples. Better: fit GP to get smooth interpolated curves.\n\n```python\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, WhiteKernel\n\nfor each band:\n    kernel = RBF() + WhiteKernel()\n    gp = GaussianProcessRegressor(kernel=kernel)\n    gp.fit(time.reshape(-1,1), flux)\n    \n    # Interpolate to regular 1-day grid\n    regular_grid = np.arange(time.min(), time.max(), 1.0)\n    flux_interp, flux_std = gp.predict(regular_grid.reshape(-1,1), return_std=True)\n    \n    # Now extract features from smooth curve\n    # - Peak location/value with uncertainty\n    # - Rise/decay times\n    # - Power-law fits on interpolated data\n```\n\n### FATS Library Integration\nInstall and use FATS (Feature Analysis for Time Series):\n```bash\npip install feets  # or FATS\n```\n\n50+ proven astronomical features:\n- Stetson indices (J, K, L)\n- Beyond1Std\n- Amplitude\n- AndersonDarling\n- Autocor_length\n- CAR parameters (damped random walk)\n- Con (fraction of consecutive same-sign changes)\n- Eta_e\n- FluxPercentileRatios\n- LinearTrend\n- MaxSlope\n- MedianAbsDev\n- MedianBRP\n- PairSlopeTrend\n- PercentDifferenceFluxPercentile\n- Rcs (range of cumulative sum)\n- Skew, Std, Mean\n- StetsonK\n- etc.\n\n### Better GBM with New Features\nAfter adding GP + FATS features, retrain LightGBM with Optuna tuning","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-23T08:56:16.790469231+11:00","updated_at":"2025-12-23T10:35:56.154155494+11:00","closed_at":"2025-12-23T10:35:56.154155494+11:00","close_reason":"GP interpolation and FATS features implemented. Added --use_gp and --use_fats flags to train_gbm.py. 472 total features available.","labels":["feature-engineering","modeling","phase:2"],"dependencies":[{"issue_id":"kaggle_mallorn-2lp","depends_on_id":"kaggle_mallorn-4k8","type":"blocks","created_at":"2025-12-23T08:59:47.471899463+11:00","created_by":"daemon"}]}
{"id":"kaggle_mallorn-4k8","title":"Phase 1: Physics-Informed Feature Engineering","description":"## Expected Impact: 0.1 → 0.25\n\nTDEs have distinctive physical signatures not being captured:\n\n### Power-Law Decay Fitting\n- Fit F ∝ t^α post-peak for each band\n- Extract α (TDEs cluster around α ≈ -1.67) and χ² goodness-of-fit\n- Current `decay_rate` is too simplistic\n\n### Bazin Function Fits  \nStandard transient parameterization:\n```\nF(t) = A * exp(-(t-t0)/τ_fall) / (1 + exp(-(t-t0)/τ_rise)) + B\n```\nExtract: amplitude, rise time, fall time, plateau\n\n### Color Evolution Features\n- d(g-r)/dt, d(r-i)/dt over time\n- TDEs show monotonic blue → red evolution\n- Compute color at peak vs color at +30 days\n\n### Smoothness Metrics\n- Second derivative statistics (TDEs are smooth)\n- Count of local maxima (TDEs have single peak)\n- Lack of secondary bumps (unlike SNe)\n\n### Band Synchronicity\n- Cross-correlation between bands\n- TDEs peak nearly simultaneously across bands\n- Time lag between band peaks\n\n## Implementation\nEdit `src/features/light_curve_features.py` to add these features","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T08:56:10.614467432+11:00","updated_at":"2025-12-23T08:56:10.614467432+11:00","labels":["feature-engineering","phase:1"],"dependencies":[{"issue_id":"kaggle_mallorn-4k8","depends_on_id":"kaggle_mallorn-5j6","type":"blocks","created_at":"2025-12-23T08:59:47.26779177+11:00","created_by":"daemon"}]}
{"id":"kaggle_mallorn-4uc","title":"Sequence representation (padding/interpolation)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T09:53:21.376743+11:00","updated_at":"2025-12-22T10:07:57.131562+11:00","closed_at":"2025-12-22T10:07:57.131562+11:00"}
{"id":"kaggle_mallorn-5j6","title":"Improve F1 Score from 0.1 to 0.7","description":"Master epic for all improvements to the MALLORN TDE classification pipeline.\n\n## Baseline Results\n| Model | CV F1 | Test F1 | Date |\n|-------|-------|---------|------|\n| Transformer ensemble (5-fold) | 0.191 | **0.085** | 2025-12-23 |\n\n## Current State\n- Training data: 3,043 objects (148 TDEs = 4.9%)\n- Test data: 7,135 objects\n- Best model: Transformer with focal loss\n\n## Key Challenges\n1. Severe class imbalance (~5% positive)\n2. Large CV-to-test gap (overfitting)\n3. Irregular time series sampling\n4. Limited physics-informed features\n\n## Target\n- Leaderboard F1 ~0.7","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T08:55:16.650192658+11:00","updated_at":"2025-12-23T13:39:20.016509444+11:00","labels":["epic","priority:high"]}
{"id":"kaggle_mallorn-5ji","title":"Monitor ROCKET submission score on Kaggle leaderboard","description":"ROCKET ensemble (CV F1=0.39±0.08) submitted to Kaggle. Currently pending scoring. Check result when available and compare to baseline (best so far: 0.101). Submission uses 5-fold Ridge classifier with 1000 kernels per band, linear interpolation to 100 time points.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-24T00:32:48.503909378+11:00","updated_at":"2025-12-24T00:32:48.503909378+11:00","labels":["kaggle","monitoring","rocket"]}
{"id":"kaggle_mallorn-8hy","title":"1D CNN model with per-band encoders","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T10:08:09.366184+11:00","updated_at":"2025-12-22T10:23:48.484415+11:00","closed_at":"2025-12-22T10:23:48.484415+11:00"}
{"id":"kaggle_mallorn-99a","title":"Transformer-based classifier","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T10:08:15.126177+11:00","updated_at":"2025-12-22T10:23:48.485721+11:00","closed_at":"2025-12-22T10:23:48.485721+11:00"}
{"id":"kaggle_mallorn-a6u","title":"PyTorch Dataset class for light curves","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T09:53:16.15364+11:00","updated_at":"2025-12-22T10:07:57.130709+11:00","closed_at":"2025-12-22T10:07:57.130709+11:00"}
{"id":"kaggle_mallorn-dr0","title":"Data loader for all 20 splits","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T09:53:05.738037+11:00","updated_at":"2025-12-22T10:07:57.129206+11:00","closed_at":"2025-12-22T10:07:57.129206+11:00"}
{"id":"kaggle_mallorn-ebp","title":"Phase 3: ROCKET + Template Matching","description":"## Phase 3: ROCKET + Template Matching\n\n**ROCKET: COMPLETED ✓**\n- Trained 5-fold ROCKET ensemble with 1000 kernels per band\n- CV F1: 0.387 ± 0.085\n- Submitted to Kaggle (pending scoring as of 2025-12-23)\n- Implementation: scripts/train_rocket.py, scripts/predict_rocket.py\n- Uses linear interpolation to 100 time points per band\n- Ridge classifier with optimized thresholds per fold\n\n**Template Matching: TODO**\n- Create template library for TDE, SNe, AGN\n- Fit templates and extract chi-squared features\n- Expected to provide complementary signal to ROCKET\n\nNext: Wait for ROCKET leaderboard score, then implement template matching if needed.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T08:57:29.712756542+11:00","updated_at":"2025-12-24T00:32:54.092019873+11:00","labels":["modeling","phase:3"],"dependencies":[{"issue_id":"kaggle_mallorn-ebp","depends_on_id":"kaggle_mallorn-2lp","type":"blocks","created_at":"2025-12-23T08:59:47.642119141+11:00","created_by":"daemon"}]}
{"id":"kaggle_mallorn-h07","title":"Training loop with focal loss","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T10:08:20.330891+11:00","updated_at":"2025-12-22T10:23:48.486009+11:00","closed_at":"2025-12-22T10:23:48.486009+11:00"}
{"id":"kaggle_mallorn-iga","title":"Cross-validation and threshold optimization","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T10:08:25.568091+11:00","updated_at":"2025-12-22T10:26:19.771507+11:00","closed_at":"2025-12-22T10:26:19.771507+11:00"}
{"id":"kaggle_mallorn-ivl","title":"Phase 6: Ensemble + Stacking + Hyperparameter Tuning","description":"## Expected Impact: 0.62 → 0.70\n\n### Diverse Model Ensemble\n\n```\nLevel 1 - Base Models (train on folds, predict OOF):\n├── Neural Networks (on raw sequences)\n│   ├── CNN (current)\n│   ├── Transformer (current)\n│   ├── LSTM with attention\n│   └── Time-aware Transformer\n│\n├── Tree-based (on tabular features)\n│   ├── LightGBM (current)\n│   ├── XGBoost\n│   ├── CatBoost\n│   └── Random Forest\n│\n├── Specialized\n│   ├── ROCKET + Ridge (on GP-interpolated)\n│   └── Template matching scores + Logistic Regression\n│\nLevel 2 - Meta-Learner:\n└── LightGBM or Logistic Regression on OOF predictions\n```\n\n### Stacking Implementation\n```python\nfrom sklearn.model_selection import StratifiedKFold\n\n# Collect OOF predictions from all models\noof_predictions = {}  # model_name -\u003e (n_samples,) array\n\nfor model_name, model in base_models.items():\n    oof_preds = np.zeros(len(y_train))\n    \n    for train_idx, val_idx in skf.split(X, y):\n        model.fit(X[train_idx], y[train_idx])\n        oof_preds[val_idx] = model.predict_proba(X[val_idx])[:, 1]\n    \n    oof_predictions[model_name] = oof_preds\n\n# Stack into meta-features\nmeta_X = np.column_stack(list(oof_predictions.values()))\n\n# Train meta-learner\nmeta_model = LogisticRegression()\nmeta_model.fit(meta_X, y_train)\n\n# For test predictions: average base model predictions, then apply meta\n```\n\n### Optuna Hyperparameter Tuning\n```python\nimport optuna\n\ndef objective(trial):\n    # Neural network params\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n    hidden_dim = trial.suggest_int('hidden_dim', 64, 256)\n    n_layers = trial.suggest_int('n_layers', 2, 6)\n    \n    # Loss params\n    focal_alpha = trial.suggest_float('focal_alpha', 0.5, 0.9)\n    focal_gamma = trial.suggest_float('focal_gamma', 1.0, 5.0)\n    \n    # Threshold\n    threshold = trial.suggest_float('threshold', 0.1, 0.5)\n    \n    model = build_model(lr, dropout, hidden_dim, n_layers)\n    \n    # 5-fold CV\n    f1_scores = []\n    for fold in range(5):\n        train_model(model, fold, focal_alpha, focal_gamma)\n        f1 = evaluate(model, fold, threshold)\n        f1_scores.append(f1)\n    \n    return np.mean(f1_scores)\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n```\n\n### Calibration + Threshold Optimization\n```python\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.isotonic import IsotonicRegression\n\n# Platt scaling or isotonic regression\ncalibrator = IsotonicRegression(out_of_bounds='clip')\ncalibrator.fit(oof_probs, y_train)\ncalibrated_probs = calibrator.transform(test_probs)\n\n# Optimize threshold for F1\nbest_f1, best_thresh = 0, 0.5\nfor thresh in np.arange(0.05, 0.95, 0.01):\n    preds = (oof_probs \u003e= thresh).astype(int)\n    f1 = f1_score(y_train, preds)\n    if f1 \u003e best_f1:\n        best_f1, best_thresh = f1, thresh\n```\n\n### Test-Time Augmentation\n```python\ndef predict_with_tta(model, lc, n_aug=10):\n    preds = []\n    for _ in range(n_aug):\n        aug_lc = augment(lc)  # Different augmentation each time\n        pred = model.predict_proba(aug_lc)\n        preds.append(pred)\n    return np.mean(preds)  # Average predictions\n```","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T08:58:26.772886257+11:00","updated_at":"2025-12-23T08:58:26.772886257+11:00","labels":["ensemble","phase:6","tuning"],"dependencies":[{"issue_id":"kaggle_mallorn-ivl","depends_on_id":"kaggle_mallorn-m1b","type":"blocks","created_at":"2025-12-23T08:59:48.522073941+11:00","created_by":"daemon"}]}
{"id":"kaggle_mallorn-m1b","title":"Phase 5: Data Augmentation + Semi-Supervised Learning","description":"## Expected Impact: 0.55 → 0.62\n\n### Light Curve Augmentation\n\n```python\nclass LightCurveAugmentation:\n    def __init__(self, p=0.5):\n        self.p = p\n    \n    def __call__(self, time, flux, flux_err):\n        if random.random() \u003e self.p:\n            return time, flux, flux_err\n            \n        # 1. Time shift (translate in time)\n        time = time + np.random.uniform(-100, 100)\n        \n        # 2. Time stretch (simulate different evolution rates)\n        stretch = np.random.uniform(0.8, 1.2)\n        time = time * stretch\n        \n        # 3. Flux scaling (simulate different distances/luminosities)\n        scale = np.random.uniform(0.7, 1.3)\n        flux = flux * scale\n        flux_err = flux_err * scale\n        \n        # 4. Noise injection (within error bars)\n        noise = np.random.normal(0, 1, len(flux)) * flux_err * 0.5\n        flux = flux + noise\n        \n        # 5. Observation dropout (simulate missing data)\n        if random.random() \u003c 0.3:\n            keep_frac = np.random.uniform(0.7, 0.95)\n            keep_idx = np.random.choice(len(time), int(len(time)*keep_frac), replace=False)\n            keep_idx = np.sort(keep_idx)\n            time, flux, flux_err = time[keep_idx], flux[keep_idx], flux_err[keep_idx]\n        \n        # 6. Redshift augmentation (simulate different z)\n        if random.random() \u003c 0.2:\n            z_shift = np.random.uniform(-0.1, 0.1)\n            flux = flux * (1 + z_shift)**2\n            time = time * (1 + z_shift)\n        \n        return time, flux, flux_err\n```\n\n### Mixup for Light Curves\n```python\ndef mixup_lightcurves(lc1, lc2, y1, y2, alpha=0.2):\n    lam = np.random.beta(alpha, alpha)\n    # Interpolate both to common grid first\n    mixed_flux = lam * lc1_interp + (1 - lam) * lc2_interp\n    mixed_y = lam * y1 + (1 - lam) * y2\n    return mixed_flux, mixed_y\n```\n\n### Semi-Supervised: Pseudo-Labeling\nUse 7,135 unlabeled test objects:\n\n```python\n# 1. Train on labeled data\nmodel.fit(X_train, y_train)\n\n# 2. Predict on unlabeled test data\nprobs = model.predict_proba(X_test)\n\n# 3. Select high-confidence predictions\nconfident_mask = (probs \u003e 0.95) | (probs \u003c 0.05)\npseudo_labels = (probs[confident_mask] \u003e 0.5).astype(int)\n\n# 4. Add to training set and retrain\nX_extended = np.vstack([X_train, X_test[confident_mask]])\ny_extended = np.hstack([y_train, pseudo_labels])\nmodel.fit(X_extended, y_extended)\n\n# 5. Iterate 2-3 times\n```\n\n### Contrastive Pre-training\n```python\nclass ContrastiveLoss(nn.Module):\n    '''SimCLR-style contrastive learning'''\n    def forward(self, z1, z2, temperature=0.5):\n        # z1, z2: embeddings of augmented versions of same LC\n        z1 = F.normalize(z1, dim=1)\n        z2 = F.normalize(z2, dim=1)\n        \n        similarity = torch.mm(z1, z2.T) / temperature\n        labels = torch.arange(len(z1))\n        \n        return F.cross_entropy(similarity, labels)\n\n# Pre-train encoder on all data (train + test)\nfor lc in all_lightcurves:\n    aug1 = augment(lc)\n    aug2 = augment(lc)\n    z1, z2 = encoder(aug1), encoder(aug2)\n    loss = contrastive_loss(z1, z2)\n```","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T08:58:24.399598534+11:00","updated_at":"2025-12-23T08:58:24.399598534+11:00","labels":["augmentation","phase:5","semi-supervised"],"dependencies":[{"issue_id":"kaggle_mallorn-m1b","depends_on_id":"kaggle_mallorn-qdv","type":"blocks","created_at":"2025-12-23T08:59:48.154833819+11:00","created_by":"daemon"}]}
{"id":"kaggle_mallorn-qdv","title":"Phase 4: Advanced Neural Architectures","description":"## Expected Impact: 0.50 → 0.55\n\n### Time-Aware Transformer\nCurrent transformer uses standard positional encoding (sequence position). Should encode actual observation times:\n\n```python\nclass TimeAwarePositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_timescale=10000):\n        super().__init__()\n        self.d_model = d_model\n        self.max_timescale = max_timescale\n        \n    def forward(self, x, times):\n        # times: actual MJD values (B, seq_len)\n        # Normalize to [0, 1] range\n        t_norm = (times - times.min()) / (times.max() - times.min() + 1e-8)\n        \n        # Fourier features at multiple frequencies\n        freqs = torch.exp(torch.linspace(0, -np.log(self.max_timescale), self.d_model//2))\n        angles = t_norm.unsqueeze(-1) * freqs\n        \n        pe = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n        return x + pe\n```\n\n### Neural ODE / Neural CDE\nPurpose-built for irregular time series:\n\n```python\nimport torchdiffeq\n\nclass NeuralODE(nn.Module):\n    def __init__(self, hidden_dim):\n        self.func = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim)\n        )\n    \n    def forward(self, x0, times):\n        # Solve ODE: dx/dt = f(x, t)\n        return torchdiffeq.odeint(self.func, x0, times)\n```\n\nLibraries: `torchdiffeq`, `torchcde`\n\n### Set Transformer\nTreat light curve as unordered set of observations:\n\n```python\n# Input: set of (time, flux, flux_err, band_onehot) tuples\n# No positional encoding needed - time is a feature\nclass SetTransformer(nn.Module):\n    def __init__(self):\n        self.encoder = ISAB(dim_in, dim_out, num_heads, num_inds)\n        self.decoder = PMA(dim, num_heads, num_outputs=1)\n```\n\n### Multi-Band Attention\nCross-attention between bands to capture color evolution:\n\n```python\nclass CrossBandAttention(nn.Module):\n    '''Let each band attend to other bands'''\n    def forward(self, band_embeddings):\n        # band_embeddings: (batch, 6, dim)\n        # Self-attention across bands\n        attn_out = self.multihead_attn(\n            band_embeddings, band_embeddings, band_embeddings\n        )\n        return attn_out\n```","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T08:57:38.519135435+11:00","updated_at":"2025-12-23T08:57:38.519135435+11:00","labels":["modeling","neural-network","phase:4"],"dependencies":[{"issue_id":"kaggle_mallorn-qdv","depends_on_id":"kaggle_mallorn-ebp","type":"blocks","created_at":"2025-12-23T08:59:47.870796525+11:00","created_by":"daemon"}]}
{"id":"kaggle_mallorn-ruv","title":"DataLoader with stratified sampling","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T09:53:26.582334+11:00","updated_at":"2025-12-22T10:07:57.131746+11:00","closed_at":"2025-12-22T10:07:57.131746+11:00"}
